{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ace467",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9376a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "def load_data():\n",
    "    fake = pd.read_csv(r\"C:\\Users\\DELL\\Videos\\ml\\Fake.csv\")\n",
    "    true = pd.read_csv(r\"C:\\Users\\DELL\\Videos\\ml\\True.csv\")\n",
    "    fake['label'] = 0  # 0 = Fake\n",
    "    true['label'] = 1  # 1 = Real\n",
    "    df = pd.concat([fake, true], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "    df = df[['title', 'label']].rename(columns={'title': 'text'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21346c47",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Step 2: Clean Text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd217c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Step 3: Visualizations\n",
    "def plot_class_distribution(df):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(data=df, x='label')\n",
    "    plt.xticks([0, 1], ['Fake', 'Real'])\n",
    "    plt.title(\"Class Distribution: Fake vs Real\")\n",
    "    plt.savefig(\"class_distribution.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e1a38",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_wordclouds(df):\n",
    "    fake_words = \" \".join(df[df['label'] == 0]['text'])\n",
    "    real_words = \" \".join(df[df['label'] == 1]['text'])\n",
    "\n",
    "    wordcloud_fake = WordCloud(background_color='black').generate(fake_words)\n",
    "    wordcloud_real = WordCloud(background_color='white').generate(real_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_fake, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake News WordCloud\")\n",
    "    plt.savefig(\"wordcloud_fake.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_real, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Real News WordCloud\")\n",
    "    plt.savefig(\"wordcloud_real.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18876625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train & Compare Models\n",
    "def train_and_compare(df):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    df['cleaned'] = df['text'].apply(clean_text)\n",
    "    df['polarity'] = df['cleaned'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    # Vectorize only the text\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_text = tfidf.fit_transform(df['cleaned'])\n",
    "\n",
    "    # Split data BEFORE combining polarity (to avoid issues with Naive Bayes)\n",
    "    X_train_text, X_test_text, y_train, y_test, train_polarity, test_polarity = train_test_split(\n",
    "        X_text, df['label'], df['polarity'], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # For other models, combine polarity\n",
    "    from scipy.sparse import hstack\n",
    "    X_train = hstack([X_train_text, np.array(train_polarity).reshape(-1, 1)])\n",
    "    X_test = hstack([X_test_text, np.array(test_polarity).reshape(-1, 1)])\n",
    "\n",
    "    # === Model 1: Logistic Regression\n",
    "    print(\"üî∏ Training Logistic Regression...\")\n",
    "    lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "    print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "\n",
    "    # === Model 2: Random Forest\n",
    "    print(\"üå≤ Training Random Forest...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', max_depth=20, n_jobs=-1, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    print(f\"Random Forest Accuracy: {acc_rf:.4f}\")\n",
    "\n",
    "    # === Model 3: Tuned Random Forest\n",
    "    print(\"üîç Tuning Random Forest with GridSearchCV...\")\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None]\n",
    "    }\n",
    "    grid = GridSearchCV(RandomForestClassifier(class_weight='balanced'), param_grid, cv=3, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    acc_best = accuracy_score(y_test, y_pred_best)\n",
    "    print(f\"‚úÖ Tuned RF Accuracy: {acc_best:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "    # === Model 4: Naive Bayes (uses only TF-IDF)\n",
    "    print(\"üß™ Training Naive Bayes...\")\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_text, y_train)\n",
    "    y_pred_nb = nb.predict(X_test_text)\n",
    "    acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "    print(f\"Naive Bayes Accuracy: {acc_nb:.4f}\")\n",
    "\n",
    "    # === Confusion Matrix (for best RF)\n",
    "    cm = confusion_matrix(y_test, y_pred_best)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "    plt.title(\"Confusion Matrix (Best RF)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # === Feature Importance (Best RF)\n",
    "    importances = best_model.feature_importances_\n",
    "    indices = np.argsort(importances)[-10:][::-1]\n",
    "    feature_names = list(tfidf.get_feature_names_out())\n",
    "    feature_names.append(\"polarity\")\n",
    "    top_features = [feature_names[i] for i in indices]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=importances[indices], y=top_features)\n",
    "    plt.title(\"Top 10 Important Features (Random Forest)\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # === Save best model & vectorizer\n",
    "    joblib.dump(best_model, \"fake_news_model.pkl\")\n",
    "    joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "    print(\"‚úÖ Model and vectorizer saved successfully!\")\n",
    "\n",
    "    # === Model Comparison Table\n",
    "    metrics = {\n",
    "        \"Model\": [\"Logistic Regression\", \"Random Forest\", \"Tuned Random Forest\", \"Naive Bayes\"],\n",
    "        \"Accuracy\": [\n",
    "            accuracy_score(y_test, y_pred_lr),\n",
    "            accuracy_score(y_test, y_pred_rf),\n",
    "            accuracy_score(y_test, y_pred_best),\n",
    "            accuracy_score(y_test, y_pred_nb)\n",
    "        ],\n",
    "        \"Precision\": [\n",
    "            precision_score(y_test, y_pred_lr),\n",
    "            precision_score(y_test, y_pred_rf),\n",
    "            precision_score(y_test, y_pred_best),\n",
    "            precision_score(y_test, y_pred_nb)\n",
    "        ],\n",
    "        \"Recall\": [\n",
    "            recall_score(y_test, y_pred_lr),\n",
    "            recall_score(y_test, y_pred_rf),\n",
    "            recall_score(y_test, y_pred_best),\n",
    "            recall_score(y_test, y_pred_nb)\n",
    "        ],\n",
    "        \"F1 Score\": [\n",
    "            f1_score(y_test, y_pred_lr),\n",
    "            f1_score(y_test, y_pred_rf),\n",
    "            f1_score(y_test, y_pred_best),\n",
    "            f1_score(y_test, y_pred_nb)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    comparison_df = pd.DataFrame(metrics)\n",
    "    print(\"\\nüìä Model Comparison Table:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    comparison_df.to_csv(\"model_comparison.csv\", index=False)\n",
    "\n",
    "    # === Plot\n",
    "    comparison_df.set_index(\"Model\")[[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]].plot(\n",
    "        kind='bar', colormap='Set2', figsize=(10, 6), ylim=(0, 1.05)\n",
    "    )\n",
    "    plt.title(\"Model Performance Comparison\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_performance_comparison.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ee5f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Main Pipeline\n",
    "def main():\n",
    "    print(\"üöÄ Starting Fake News Detection Training...\")\n",
    "    df = load_data()\n",
    "    plot_class_distribution(df)\n",
    "    plot_wordclouds(df)\n",
    "    train_and_compare(df)\n",
    "    print(\"üéâ Training pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
